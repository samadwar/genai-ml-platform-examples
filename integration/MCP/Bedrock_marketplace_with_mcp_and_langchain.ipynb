{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f86c282-14f5-4fdb-90ba-1fa7df834b04",
   "metadata": {},
   "source": [
    "## Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2096d34a-857a-49a0-add0-9bf2c3868586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mcp langchain-aws langchain --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378b03d-9d42-482a-9133-fe555af2c16d",
   "metadata": {},
   "source": [
    "# MCP\n",
    "Model Context Protocol (MCP) enables seamless integration between LLM applications and external data sources and tools. It is made up of two main components:\n",
    "- MCP Server\n",
    "- MCP Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2e703-5c45-4087-bb50-4ea83c612a75",
   "metadata": {},
   "source": [
    "## Server\n",
    "First, let's create an MCP server that check the weather in a given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7888d473-4449-4bf5-a27a-05a2e7ec04d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_server.py\n",
    "from typing import List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather for location.\"\"\"\n",
    "    return f\"It's sunny in {location}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f11c1-df5a-40f3-b6ea-90cc12c834db",
   "metadata": {},
   "source": [
    "## Client\n",
    "Now, let's define a client class that has two methods (`list_tools` and `execute_tool`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc7729b-2b50-4435-8d58-134225b624a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"/home/ec2-user/SageMaker/weather_server.py\"],\n",
    ")\n",
    "\n",
    "\n",
    "class MCP_client:\n",
    "    def __init__(self, server_params):\n",
    "        self.server_params = server_params\n",
    "        self.operation = {\n",
    "            \"list_tools\": self.list_tools,\n",
    "            \"execute_tool\": self.execute_tool\n",
    "        }\n",
    "\n",
    "    async def list_tools(self):\n",
    "        async with stdio_client(self.server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                await session.initialize()\n",
    "                # Get tools\n",
    "                tools = await session.list_tools()\n",
    "                return \"[\"+\",\\n\".join([tool.model_dump_json(indent=4) for tool in tools.tools]) + \"]\", False\n",
    "\n",
    "    async def execute_tool(self, tool_name, tool_parameters):\n",
    "        async with stdio_client(self.server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                await session.initialize()\n",
    "                # Call tools\n",
    "                result = await session.call_tool(tool_name, tool_parameters)\n",
    "                return result.content[0].text, result.isError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb863f-290e-4401-823e-6573f5d8808f",
   "metadata": {},
   "source": [
    "### Let's test our MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f77f5f-a5ef-4b92-b036-2351622abaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling list_tools operation\n",
      "[{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"description\": \"Get weather for location.\",\n",
      "    \"inputSchema\": {\n",
      "        \"properties\": {\n",
      "            \"location\": {\n",
      "                \"title\": \"Location\",\n",
      "                \"type\": \"string\"\n",
      "            }\n",
      "        },\n",
      "        \"required\": [\n",
      "            \"location\"\n",
      "        ],\n",
      "        \"title\": \"get_weatherArguments\",\n",
      "        \"type\": \"object\"\n",
      "    }\n",
      "}]\n",
      "\n",
      "Calling execute_tool operation\n",
      "It's sunny in Sydney\n"
     ]
    }
   ],
   "source": [
    "weather_client = MCP_client(server_params)\n",
    "print(\"Calling list_tools operation\")\n",
    "print((await weather_client.list_tools())[0])\n",
    "print()\n",
    "print(\"Calling execute_tool operation\")\n",
    "print((await weather_client.execute_tool(\"get_weather\", {\"location\": \"Sydney\"}))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a89e7a-f5e5-4ac5-a538-ded23ea28c44",
   "metadata": {},
   "source": [
    "# Using MCP with Bedrock Marketplace models\n",
    "\n",
    "Here we are going to illustrate how to use bedrock marketplace models with MCP.\n",
    "Make sure to deploy your model from Bedrock Marketplace console first. See [here](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-marketplace-deploy-a-model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a17687-adca-4224-9a98-008c331c036f",
   "metadata": {},
   "source": [
    "## Defining some helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df70164-8dfb-4716-884a-c10ad6412c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def is_valid_json(text):\n",
    "    # Find the text between the first and last curly brackets\n",
    "    match = re.search(r'\\{([\\s\\S]*)\\}', text)\n",
    "    if not match:\n",
    "        return {\"isJSON\": False, \"parsedJson\": None}  # No curly brackets found\n",
    "    # Extract the content between the first and last curly brackets\n",
    "    # We add back the curly brackets to make it a complete JSON string\n",
    "    json_candidate = '{' + match.group(1) + '}'\n",
    "    try:\n",
    "        # Try to parse the string as JSON\n",
    "        parsed_json = json.loads(json_candidate)\n",
    "        return {\"isJSON\": True, \"parsedJson\": parsed_json}  # If no error is thrown, it's valid JSON\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"isJSON\": False, \"parsedJson\": None}  # If an error is thrown, it's not valid JSON\n",
    "\n",
    "\n",
    "def is_tool_use(input_data):\n",
    "    tool_use = \"\"\n",
    "    input_message = \"\"\n",
    "    is_json = is_valid_json(input_data)\n",
    "    if is_json[\"isJSON\"]:\n",
    "        parsed_json = is_json[\"parsedJson\"]\n",
    "        if \"operation\" in parsed_json:\n",
    "            tool_use = parsed_json\n",
    "        else:\n",
    "            input_message = input_data\n",
    "    else:\n",
    "        input_message = input_data\n",
    "    return {\n",
    "            \"tool_use\": tool_use,\n",
    "            \"inputMessage\": input_message\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39831967-c34d-49e7-bdd3-8be14c4e085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_llm_response(response):\n",
    "    tool_use_response = is_tool_use(response)\n",
    "    tool_call_id = uuid.uuid4()\n",
    "    if tool_use_response[\"inputMessage\"]:\n",
    "        return {\"AI\": tool_use_response[\"inputMessage\"]}\n",
    "    elif tool_use_response[\"tool_use\"][\"operation\"] in weather_client.operation:\n",
    "        kwargs = {}\n",
    "        if \"tool_name\" in tool_use_response[\"tool_use\"]:\n",
    "            kwargs[\"tool_name\"] = tool_use_response[\"tool_use\"][\"tool_name\"]\n",
    "        if \"tool_parameters\" in tool_use_response[\"tool_use\"]:\n",
    "            kwargs[\"tool_parameters\"] = tool_use_response[\"tool_use\"][\"tool_parameters\"]\n",
    "        result = await weather_client.operation[tool_use_response[\"tool_use\"][\"operation\"]](**kwargs)\n",
    "        if result[1]:\n",
    "            return {\"Tool\": f\"Error: {result[0]}\", \"call_id\": tool_call_id}\n",
    "        else:\n",
    "            return {\"Tool\": result[0], \"call_id\": tool_call_id}\n",
    "    else:\n",
    "        return {\"Tool\": \"NoSuchOperationError: Operation not supported. Did you mean to use execute_tool operation with tool_name? reformat your JSON correctly and try again.\", \"call_id\": tool_call_id}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765c40b-48a8-4d4f-9fe1-aea6f2e80ce7",
   "metadata": {},
   "source": [
    "## LangChain and Bedrock\n",
    "Initializing langchain Chat class for Bedrock. Make sure to provide SageMaker AI endpoint ARN as your Bedrock `model_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a435e41-2f2e-44be-88f8-7f149f436ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain.schema.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "region = \"us-west-2\"\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "model_id = \"<sagemaker endpoint ARN>\"\n",
    "provider = \"amazon\" # Works for Qwen2.5 14B Instruct\n",
    "\n",
    "bedrock_client = ChatBedrockConverse(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    "    provider=provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd786a-c1ee-4c75-bb55-c83a0334e58a",
   "metadata": {},
   "source": [
    "### System prompt\n",
    "We want to make the LLM know that if they want to access a tool they should call `list_tools` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c4de10-12d3-480a-943f-4ebf805dea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "systemMessage = \"\"\"You are a helpful assistance.\n",
    "You have access to perform operation calls, when not performing an operation call, always respond with unstructred text.\n",
    "When you need to perform an operation call, you must respond with JSON format only. Always start with 'ChainOfThoughs' attribute. You can use 'ChainOfThoughs' attribute when performing an operation to tell me your thought process. Also use 'ChainOfThoughs' attribute to talk to me when performing an operation. Do not generate any text outside of the JSON format.\n",
    "ALWAYS call list_tool operation before performing any execute_tool operation\n",
    "You have access to the following operations:\n",
    "<Tools/>\n",
    "[\n",
    "{\n",
    "  'operation': 'list_tools',\n",
    "  'description': 'Use this operation to list all tools that you have access to.'\n",
    "  }\n",
    "},\n",
    "{\n",
    "  'operation': 'execute_tool'\n",
    "  'description': 'Use this operation to execute a tool. Only tools listed in the list_tools operation can be executed. You can get tool_name and tool_parameters from the list_tools response',\n",
    "  'tool_name': '<the tool name you get from list_tools>',\n",
    "  'tool_parameters': '<the tool parameters to supply as per list_tools>'\n",
    "  }\n",
    "}\n",
    "]\n",
    "</Tools>\n",
    "\"\"\"\n",
    "\n",
    "question = \"What's the weather in New York?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2da64-3606-4425-959b-bdf6fa387fb7",
   "metadata": {},
   "source": [
    "### Invocation logic\n",
    "The `handle_llm_response` function will decide if the AI is calling a tool operation or it is just responding with normal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145ecdc1-17bd-41f3-b54f-e5d8342c5c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in New York is currently sunny.\n",
      "\n",
      "\n",
      "### CHAT HISTORY ###\n",
      "\n",
      "User: What's the weather in New York?\n",
      "\n",
      "AI: {\n",
      "  \"operation\": \"list_tools\"\n",
      "}\n",
      "\n",
      "Tool: [{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"description\": \"Get weather for location.\",\n",
      "    \"inputSchema\": {\n",
      "        \"properties\": {\n",
      "            \"location\": {\n",
      "                \"title\": \"Location\",\n",
      "                \"type\": \"string\"\n",
      "            }\n",
      "        },\n",
      "        \"required\": [\n",
      "            \"location\"\n",
      "        ],\n",
      "        \"title\": \"get_weatherArguments\",\n",
      "        \"type\": \"object\"\n",
      "    }\n",
      "}]\n",
      "\n",
      "AI: {\n",
      "  \"operation\": \"execute_tool\",\n",
      "  \"tool_name\": \"get_weather\",\n",
      "  \"tool_parameters\": {\n",
      "    \"location\": \"New York\"\n",
      "  },\n",
      "  \"ChainOfThoughts\": \"The user wants to know the weather in New York, so I need to use the get_weather tool with location set to New York.\"\n",
      "}\n",
      "\n",
      "Tool: It's sunny in New York\n",
      "\n",
      "AI: The weather in New York is currently sunny.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke Example\n",
    "\n",
    "# Initialize chat history\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": systemMessage },\n",
    "    { \"role\": \"user\", \"content\": question }\n",
    "]\n",
    "\n",
    "AI_responded = False\n",
    "tool_response = \"\"\n",
    "while not AI_responded:\n",
    "    # prompt = ChatPromptTemplate.from_messages(template)\n",
    "\n",
    "\n",
    "    # Create simple pipeline\n",
    "    chain = bedrock_client | StrOutputParser()\n",
    "\n",
    "    # Chain Invoke\n",
    "    response = chain.invoke(messages)\n",
    "    messages.append({\"role\": \"ai\", \"content\": response})\n",
    "    result = await handle_llm_response(response)\n",
    "    if \"AI\" in result:\n",
    "        AI_responded = True\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": result[\"Tool\"]})\n",
    "        tool_response = result[\"Tool\"]\n",
    "\n",
    "print(response)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"### CHAT HISTORY ###\")\n",
    "print()\n",
    "print(f'User: {messages[1][\"content\"]}')\n",
    "print()\n",
    "for message in messages[2:]:\n",
    "    role = \"Tool\" if message[\"role\"] == \"user\" else \"AI\"\n",
    "    print(f'{role}: {message[\"content\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f2459-591a-487b-aac4-dede903d8588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
